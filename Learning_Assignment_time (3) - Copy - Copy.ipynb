{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T  \n",
    "from scipy.stats import norm\n",
    "from itertools import combinations\n",
    "from sklearn.utils import shuffle\n",
    "from numpy import array\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN1(nn.Module):\n",
    "\n",
    "    def __init__(self,m):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.fc1 = nn.Linear(in_features=m, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.fc4 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc5 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.out = nn.Linear(in_features=64, out_features=7)\n",
    "    def forward(self, t):\n",
    "        \n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = F.relu(self.fc3(t))\n",
    "        t = F.relu(self.fc4(t))\n",
    "        t = F.relu(self.fc5(t))\n",
    "        t = self.out(t)\n",
    "        return t\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon Greedy Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "            math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "        \n",
    "    def select_action(self, state, policy_net,k,point):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step) \n",
    "        self.current_step += k\n",
    "        c = 0\n",
    "        \n",
    "        if point == 0 :\n",
    "            if rate > random.random():\n",
    "                return torch.tensor([[random.randrange(self.num_actions)]], device=device, dtype=torch.long),c,rate\n",
    "            else:\n",
    "                c = c + 1;\n",
    "                with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "                    return policy_net(state).argmax().view(1, 1),c,rate\n",
    "                #return policy_net(state).argmax().item()\n",
    "        else:\n",
    "            c = c + 1;\n",
    "            with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "                return policy_net(state).argmax().view(1, 1),c,rate\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def take_action(action1,number_action,state,h0,nTx,Es_N0,act):     \n",
    "def take_action(action1,state,h0,nTx,Es_N0,S): \n",
    "    \n",
    "    counter = 0\n",
    "    i = 0\n",
    " #   th = pow(10,-3)\n",
    "    device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\") \n",
    "    action = np.array([2,4,8,16,32,64,128])\n",
    "    action = torch.tensor([action], device=device) \n",
    "    ModulationOrder = action[0,action1]\n",
    "\n",
    "    SNR = Es_N0*pow(10,(h0-30)/10)\n",
    "    \n",
    "    if ModulationOrder == 2:\n",
    "        i = 2\n",
    "        BER = 1-(norm.cdf(math.sqrt(2*SNR)))\n",
    "    elif ModulationOrder == 4:\n",
    "        i = 4\n",
    "        BER = (2/2)*(1-(1/math.sqrt(4)))*(1-norm.cdf(math.sqrt((3*SNR)/(4-1))))\n",
    "    elif ModulationOrder == 8:\n",
    "        i = 8\n",
    "        BER = (2/3)*(1-(1/math.sqrt(8)))*(1-norm.cdf(math.sqrt((3*SNR)/(8-1))))\n",
    "    elif ModulationOrder == 16:\n",
    "        i = 16\n",
    "        BER = (2/4)*(1-(1/math.sqrt(16)))*(1-norm.cdf(math.sqrt((3*SNR)/(16-1))))\n",
    "    elif ModulationOrder == 32:\n",
    "        i = 32\n",
    "        BER = (2/5)*(1-(1/math.sqrt(32)))*(1-norm.cdf(math.sqrt((3*SNR)/(32-1))))\n",
    "    elif ModulationOrder == 64:\n",
    "        i = 64\n",
    "        BER = (2/6)*(1-(1/math.sqrt(64)))*(1-norm.cdf(math.sqrt((3*SNR)/(64-1))))\n",
    "    elif ModulationOrder == 128:\n",
    "        i = 128\n",
    "        BER = (2/7)*(1-(1/math.sqrt(128)))*(1-norm.cdf(math.sqrt((3*SNR)/(128-1))))\n",
    "\n",
    "    \n",
    "    R = S*math.log(ModulationOrder,2) * ((1-math.log(ModulationOrder,2)*BER)**S)\n",
    "\n",
    "    reward1 = R\n",
    "\n",
    "    return torch.tensor([reward1], device=device),counter,BER,ModulationOrder,SNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(2)\n",
    "    plt.clf()        \n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Time step')\n",
    "    plt.ylabel('Throughput(bits/s)')\n",
    "    plt.plot(values)\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)    \n",
    "    plt.pause(0.001)\n",
    "    print(\"Time step\", len(values), \"\\n\", \\\n",
    "            moving_avg_period, \"Time step moving avg:\", moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)\n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculating Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1 ,index=actions)\n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):                \n",
    "        final_state_locations = next_states.flatten(start_dim=1) \\\n",
    "            .max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(QValues.device)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "batch_size = 1000\n",
    "gamma = 0.001\n",
    "eps_start = 1\n",
    "eps_end = 0.0001\n",
    "eps_decay = 0.0001\n",
    "eps_th = 0.0002\n",
    "target_update = 100\n",
    "memory_size1 = 50000\n",
    "lr = 0.003\n",
    "num_episodes = 1000\n",
    "num_actions_available1 =7 \n",
    "\n",
    "\n",
    "nRx = 1\n",
    "nTx = 1\n",
    "\n",
    "mm=10\n",
    "mm_state=np.zeros((mm,nRx,nTx),np.float64)\n",
    "mm_state2=np.zeros((mm,nRx,nTx),np.float64)\n",
    "\n",
    "SNRdB = np.arange(110,111)\n",
    "\n",
    "epsilon = 1\n",
    "epsilon1 = 1\n",
    "\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NN = 10000\n",
    "S = 1000\n",
    "point = 0\n",
    "\n",
    "reward_final = []\n",
    "reward_final1 = []\n",
    "SNR_final = []\n",
    "BER_final = []\n",
    "SER_final = []\n",
    "rate_final = []\n",
    "rate1_final = []\n",
    "rate2_final = []\n",
    "ModulationOrder_final = []\n",
    "ModulationOrder_final123 = []\n",
    "\n",
    "annots = loadmat('R999_new.mat')\n",
    "h_first = annots['R']\n",
    "print((h_first.shape))\n",
    "h1 = h_first[mm-1:np.shape(h_first)[0]]\n",
    "mm_state[1:mm] = np.reshape(np.fliplr(h_first[0:mm-1]),(mm-1,1,1))\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "reward_f_con =[]\n",
    "reward_f = []\n",
    "for ii in range (np.shape(SNRdB)[0]):\n",
    "    Es_N0 = pow(10,(SNRdB[ii]/10))\n",
    "    print(ii)\n",
    "    point = 0\n",
    "    epsilon = 1\n",
    "    episode_durations=[]\n",
    "\n",
    "    reward_matrix = []\n",
    "    BER_matrix = []\n",
    "    SER_matrix = []\n",
    "    SNR_matrix = []\n",
    "    rate_rate = []\n",
    "    rate_rate1 = []\n",
    "    rate_rate2 = []\n",
    "    reward_matrix_con=[]\n",
    "    reward_matrix = []\n",
    "    NETW = []\n",
    "    ModulationOrder_matrix = []\n",
    "    \n",
    "    strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "    agent = Agent(strategy,num_actions_available1 , device)\n",
    "    memory1 = ReplayMemory(memory_size1)\n",
    "\n",
    "    policy_net1 = DQN1(mm).to(device)    \n",
    "    target_net1 = DQN1(mm).to(device)\n",
    "\n",
    "    target_net1.load_state_dict(policy_net1.state_dict())\n",
    "    target_net1.eval()\n",
    "    optimizer1 = optim.Adam(params=policy_net1.parameters(), lr=lr)\n",
    "    \n",
    "    for episode in range(0,1):\n",
    "\n",
    "        strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "        agent = Agent(strategy,num_actions_available1 , device)\n",
    "        memory1 = ReplayMemory(memory_size1)\n",
    "        \n",
    "        target_net1.load_state_dict(policy_net1.state_dict())\n",
    "        target_net1.eval()\n",
    "\n",
    "        optimizer1 = optim.Adam(params=policy_net1.parameters(), lr=lr)\n",
    "\n",
    "        mm_state[1:mm] = np.reshape(np.fliplr(h_first[0:mm-1]),(mm-1,1,1))\n",
    "        i = 0\n",
    "        k = 1\n",
    "        h0 = h1[i]\n",
    "        state0 = h1[i]\n",
    "\n",
    "        mm_state[0]=state0\n",
    "        state =torch.tensor([mm_state],dtype=torch.float32, device=device)\n",
    "        point = 0\n",
    "        epsilon = 1\n",
    "            \n",
    "        for timestep in range(np.shape(h1)[0]-1):\n",
    "\n",
    "            i = i+1\n",
    "            action,c,epsilon = agent.select_action(state,policy_net1,k,point)\n",
    "            h0 = h1[i]\n",
    "            next_state0 = h1[i] \n",
    "            mm_state = np.roll(mm_state,1,axis=0)\n",
    "            mm_state[0] = next_state0\n",
    "            next_state = torch.tensor([mm_state],dtype=torch.float32, device=device)\n",
    "            \n",
    "            reward,counter,BER,ModulationOrder,SNRR = take_action(action,next_state,h0,nTx,Es_N0,S)\n",
    "            memory1.push(Experience(state, action, next_state, reward))\n",
    "            state = next_state\n",
    "            \n",
    "            reward_matrix.append(reward)\n",
    "\n",
    "            \n",
    "            if epsilon < eps_th and point == 0:\n",
    "                point = point+1\n",
    "        \n",
    "\n",
    "            if point == 0:\n",
    "\n",
    "                if memory1.can_provide_sample(batch_size):\n",
    "                    experiences = memory1.sample(batch_size)\n",
    "                    states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "                    current_q_values = QValues.get_current(policy_net1, states, actions)\n",
    "                    next_q_values = QValues.get_next(target_net1, next_states)\n",
    "                    target_q_values = (next_q_values * gamma) + rewards\n",
    "\n",
    "                    loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "                    optimizer1.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer1.step()\n",
    "                \n",
    "                if timestep % target_update == 0:\n",
    "\n",
    "                    target_net1.load_state_dict(policy_net1.state_dict())\n",
    "                    \n",
    "        reward_final.append(reward_matrix)\n",
    "\n",
    "        np.savez(\"reward_dete2_snr22_112\", reward_final)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
